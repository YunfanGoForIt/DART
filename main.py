import os
import asyncio
import logging
import uvicorn
import datetime
from fastapi import FastAPI, BackgroundTasks, Request, Depends
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session
from contextlib import asynccontextmanager
import glob

from models import SessionLocal, init_db, ProcessedRepo, ProcessingStatus
from github_client import GitHubMonitor
from feishu_client import FeishuService
from mcp_client import DeepWikiMCPClient
from rag_refine import RAGRefiner, Config
import httpx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("mcp_deepwiki.log"),
        logging.StreamHandler()
    ],
    force=True  # override any previous logging.basicConfig (e.g., from imports)
)
logger = logging.getLogger(__name__)

# Task running flag to prevent concurrent sync tasks
is_task_running = False

# Initialize DB
init_db()

# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Service Instances
github_monitor = None
feishu_service = None
rag_refiner = None
mcp_client = None
deepwiki_indexer = None
templates = Jinja2Templates(directory="/www/wwwroot/mcp_deepwiki/templates")

config = Config()

async def process_repo_workflow(db: Session, repo_data: dict):
    repo_id = str(repo_data["id"])
    repo_name = repo_data["full_name"]
    github_url = repo_data["html_url"]
    
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    
    if db_repo:
        # Always update timestamp to show we checked it
        db_repo.updated_at = datetime.datetime.now(datetime.UTC)
        db.commit()
        
    # Skip COMPLETED, PROCESSING, and SKIPPED repos, but allow retrying FAILED and PENDING repos
    if db_repo and (db_repo.status == ProcessingStatus.COMPLETED or db_repo.status == ProcessingStatus.PROCESSING or db_repo.status == ProcessingStatus.SKIPPED):
        return
        
    if not db_repo:
        db_repo = ProcessedRepo(
            repo_id=repo_id,
            repo_name=repo_name,
            repo_url=github_url,
            description=repo_data.get("description"),
            status=ProcessingStatus.PROCESSING
        )
        db.add(db_repo)
    else:
        db_repo.status = ProcessingStatus.PROCESSING
    db.commit()
    db.refresh(db_repo)
    
    try:
        logger.info(f"üöÄ ÂºÄÂßãÂ§ÑÁêÜ‰ªìÂ∫ì: {repo_name}")
        
        safe_name = repo_name.replace("/", "_")
        base_dir = f"/www/wwwroot/mcp_deepwiki/output/{safe_name}"
        
        # 1. Fetch from DeepWiki MCP if data is missing
        if not os.path.exists(base_dir) or not glob.glob(os.path.join(base_dir, "*Overview.md")):
            logger.info(f"üì• Êï∞ÊçÆÁº∫Â§±Ôºå‰ªé DeepWiki MCP Ëé∑Âèñ: {repo_name}")
            try:
                await mcp_client.fetch_and_save(repo_name)
            except Exception as e:
                # If MCP fails, it's likely unindexed or a connection issue. Skip for now to avoid dead loops.
                raise Exception(f"MCP fetch failed: {e}. The repository might not be indexed in DeepWiki.")

        if not os.path.exists(base_dir):
            raise Exception(f"Repo data folder not found after MCP fetch: {base_dir}")

        # Find Overview file
        overview_files = glob.glob(os.path.join(base_dir, "*Overview.md"))

        # Check if it's a cold repository (only has 1 document file)
        all_md_files = glob.glob(os.path.join(base_dir, "*.md"))
        if len(all_md_files) <= 1:
            logger.warning(f"‚ö†Ô∏è ÂÜ∑Èó®‰ªìÂ∫ìÊ£ÄÊµãÔºö{repo_name} Âè™Êúâ {len(all_md_files)} ‰∏™ÊñáÊ°£ÔºåÊ†áËÆ∞‰∏∫Ë∑≥Ëøá")
            db_repo.status = ProcessingStatus.SKIPPED
            db_repo.error_message = f"ÂÜ∑Èó®‰ªìÂ∫ìÔºö‰ªÖÊúâ {len(all_md_files)} ‰∏™ÊñáÊ°£ÔºàÈúÄË¶Å Overview.mdÔºâ"
            db.commit()
            return

        if not overview_files:
            raise Exception("No Overview.md found")
        target_file = overview_files[0]

        with open(target_file, "r", encoding="utf-8") as f:
            content = f.read()

        # 2. RAG Refine
        logger.info(f"‚úçÔ∏è Ê≠£Âú®ÁîüÊàêÂàùÁ®ø...")
        # Generate Draft
        draft = await rag_refiner.generate_draft(content)
        
        # Select documents
        logger.info(f"üìö Ê≠£Âú®ÈÄâÊã©Áõ∏ÂÖ≥ÊñáÊ°£...")
        candidate_files = [os.path.basename(p) for p in glob.glob(os.path.join(base_dir, "*.md")) if os.path.abspath(p) != os.path.abspath(target_file)]
        selected_files = await rag_refiner.select_documents_for_rag(draft, candidate_files)

        # Build Knowledge Base
        logger.info(f"üß† Ê≠£Âú®ÊûÑÂª∫ÂêëÈáèÁü•ËØÜÂ∫ì...")
        await rag_refiner.build_knowledge_base(base_dir, target_file, selected_files)

        # Final Expand
        logger.info(f"üîÑ Ê≠£Âú®ÈÄöËøá RAG Êâ©Â±ïÂÜÖÂÆπ...")
        final_content = await rag_refiner.expand_with_rag(draft)
        
        # 3. Upload to Feishu
        logger.info(f"üì§ Ê≠£Âú®‰∏ä‰º†Âà∞È£û‰π¶Áü•ËØÜÂ∫ì...")
        title = f"{repo_name} RAG Refined"
        if not db_repo.feishu_doc_token:
            logger.info(f"üÜï ÂàõÂª∫Êñ∞ÁöÑÈ£û‰π¶ÊñáÊ°£ËäÇÁÇπ")
            doc_token = await feishu_service.create_node(title=title)
            if doc_token:
                db_repo.feishu_doc_token = doc_token
                db.commit()
        else:
            logger.info(f"üìù Êõ¥Êñ∞Â∑≤ÊúâÈ£û‰π¶ÊñáÊ°£")
            doc_token = db_repo.feishu_doc_token

        if doc_token:
            # Note: update_document_content currently appends content.
            # In a production scenario, you might want to clear existing blocks first.
            await feishu_service.update_document_content(doc_token, final_content)

            # 4. Notify
            logger.info(f"üîî ÂèëÈÄÅÈÄöÁü•...")
            await feishu_service.send_card_notification(
                title=f"RAG Refined Wiki: {repo_name}",
                summary=repo_data.get("description") or "Documentation optimized via RAG workflow.",
                url=f"https://feishu.cn/docx/{doc_token}"
            )
            # Add plain text webhook notification
            await feishu_service.send_webhook_notification(repo_name, doc_token)
        
        logger.info(f"‚úÖ ‰ªìÂ∫ìÂ§ÑÁêÜÂÆåÊàê: {repo_name}")
        db_repo.status = ProcessingStatus.COMPLETED
    except Exception as e:
        error_msg = str(e)
        # Check if it's a cold repository error
        is_cold_repo = (
            "No Overview.md found" in error_msg or
            ("MCP fetch failed" in error_msg and "unindexed" in error_msg.lower()) or
            ("TaskGroup" in error_msg and "sub-exception" in error_msg)
        )

        if is_cold_repo:
            logger.warning(f"‚ö†Ô∏è ÂÜ∑Èó®‰ªìÂ∫ì [{repo_name}]: {error_msg}")
            db_repo.status = ProcessingStatus.SKIPPED
            db_repo.error_message = f"ÂÜ∑Èó®‰ªìÂ∫ìÔºö{error_msg}"
        else:
            logger.error(f"‚ùå Â§ÑÁêÜÂ§±Ë¥• [{repo_name}]: {error_msg}")
            db_repo.status = ProcessingStatus.FAILED
            db_repo.error_message = error_msg
    
    db.commit()

async def sync_task(sync_all: bool = False, silent: bool = False):
    global is_task_running

    # Check if another task is already running
    if is_task_running:
        if not silent:
            logger.info(f"‚è≥ Âè¶‰∏Ä‰∏™‰ªªÂä°Ê≠£Âú®ËøêË°åÔºåË∑≥ËøáÊ≠§Ê¨°Ë∞ÉÂ∫¶")
        return

    is_task_running = True

    try:
        if not silent:
            logger.info(f"üîÑ ÂºÄÂßãÂêåÊ≠•‰ªªÂä° (sync_all={sync_all})")
        db = SessionLocal()
        try:
            # 1. Fetch new star repositories from GitHub
            logger.info(f"‚≠ê Ê≠£Âú®Ëé∑Âèñ GitHub ÊúÄÊñ∞ star...")
            stars = await github_monitor.fetch_recent_stars(limit=10)
            logger.info(f"üì¶ ÂèëÁé∞ {len(stars)} ‰∏™Êñ∞ÁöÑ star ‰ªìÂ∫ì")
            for star in stars:
                await process_repo_workflow(db, star)

            # 2. Process pending/failed repositories from database (only FAILED and PENDING, not SKIPPED)
            pending_repos = db.query(ProcessedRepo).filter(
                (ProcessedRepo.status == ProcessingStatus.PENDING) |
                (ProcessedRepo.status == ProcessingStatus.FAILED)
            ).all()

            if pending_repos:
                if not silent:
                    logger.info(f"üìã ÂèëÁé∞ {len(pending_repos)} ‰∏™ÂæÖÂ§ÑÁêÜ/Â§±Ë¥•ÁöÑÂéÜÂè≤‰ªìÂ∫ì")
            elif not silent:
                logger.info(f"‚ú® Ê≤°ÊúâÂæÖÂ§ÑÁêÜÁöÑÂéÜÂè≤‰ªìÂ∫ì")

            for repo in pending_repos:
                # Convert db record to dict format expected by process_repo_workflow
                repo_data = {
                    "id": repo.repo_id,
                    "full_name": repo.repo_name,
                    "html_url": repo.repo_url,
                    "description": repo.description
                }
                await process_repo_workflow(db, repo_data)
        finally:
            db.close()
            if not silent:
                logger.info("‚úÖ ÂêåÊ≠•‰ªªÂä°ÂÆåÊàê")
    finally:
        is_task_running = False

# Background Scheduler
async def scheduler_loop():
    while True:
        await asyncio.sleep(30) # Run every 30 seconds
        await sync_task(sync_all=False, silent=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global github_monitor, feishu_service, rag_refiner, mcp_client
    logger.info("=" * 50)
    logger.info("üöÄ MCP DeepWiki ÊúçÂä°ÂêØÂä®‰∏≠...")
    logger.info("=" * 50)

    github_monitor = GitHubMonitor(os.getenv("GITHUB_TOKEN"))
    logger.info("‚úÖ GitHub ÁõëÊéßÂô®Â∑≤ÂàùÂßãÂåñ")

    feishu_service = FeishuService(
        os.getenv("FEISHU_APP_ID"),
        os.getenv("FEISHU_APP_SECRET"),
        os.getenv("FEISHU_SPACE_ID"),
        os.getenv("FEISHU_WEBHOOK_URL")
    )
    logger.info("‚úÖ È£û‰π¶ÊúçÂä°Â∑≤ÂàùÂßãÂåñ")

    rag_refiner = RAGRefiner()
    logger.info("‚úÖ RAG Á≤æÁÇºÂô®Â∑≤ÂàùÂßãÂåñ")

    mcp_client = DeepWikiMCPClient()
    logger.info("‚úÖ DeepWiki MCP ÂÆ¢Êà∑Á´ØÂ∑≤ÂàùÂßãÂåñ")

    logger.info("‚è∞ ÂêØÂä®ÂêéÂè∞Ë∞ÉÂ∫¶Âô® (ÊØè30ÁßíÊâßË°å‰∏ÄÊ¨°)")
    asyncio.create_task(scheduler_loop())

    # Initialize DB with historical stars on first run
    db = SessionLocal()
    try:
        if db.query(ProcessedRepo).count() == 0:
            logger.info("üéØ È¶ñÊ¨°ËøêË°åÔºöÊ≠£Âú®ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÔºåÂØºÂÖ•ÊâÄÊúâÂéÜÂè≤ star ‰ªìÂ∫ì...")
            stars = await github_monitor.fetch_all_stars()
            logger.info(f"üìä ÂÖ±ÊâæÂà∞ {len(stars)} ‰∏™ star ‰ªìÂ∫ì")
            for star in stars:
                repo_id = str(star["id"])
                if not db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first():
                    # Check if we already have the output folder for this repo
                    safe_name = star["full_name"].replace("/", "_")
                    base_dir = f"/www/wwwroot/mcp_deepwiki/output/{safe_name}"

                    status = ProcessingStatus.PENDING
                    # If refined file already exists, mark as completed
                    if os.path.exists(os.path.join(base_dir, "refined", "02_Overview_Refined.md")):
                        status = ProcessingStatus.COMPLETED

                    repo = ProcessedRepo(
                        repo_id=repo_id,
                        repo_name=star["full_name"],
                        repo_url=star["html_url"],
                        description=star.get("description"),
                        status=status
                    )
                    db.add(repo)
            db.commit()
            logger.info(f"‚úÖ Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂÆåÊàêÔºåÂÖ± {len(stars)} ‰∏™‰ªìÂ∫ì")
        else:
            logger.info("‚úÖ Êï∞ÊçÆÂ∫ìÂ∑≤ÂàùÂßãÂåñÔºåË∑≥ËøáÈ¶ñÊ¨°ËøêË°åËÆæÁΩÆ")
    except Exception as e:
        logger.error(f"‚ùå Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂ§±Ë¥•: {e}")
    finally:
        db.close()

    logger.info("=" * 50)
    logger.info("üéâ MCP DeepWiki ÊúçÂä°ÂêØÂä®ÂÆåÊàêÔºÅ")
    logger.info("=" * 50)
    yield

app = FastAPI(lifespan=lifespan)

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/repos")
async def get_repos(db: Session = Depends(get_db)):
    return db.query(ProcessedRepo).order_by(ProcessedRepo.updated_at.desc()).all()

@app.post("/api/retry/{repo_id}")
async def retry_repo(repo_id: str, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    if not db_repo:
        logger.warning(f"‚ö†Ô∏è ÈáçËØïÂ§±Ë¥•Ôºö‰ªìÂ∫ì {repo_id} ‰∏çÂ≠òÂú®")
        return {"error": "Repository not found"}, 404

    logger.info(f"üîÑ Ê≠£Âú®ÈáçËØï‰ªìÂ∫ì: {db_repo.repo_name} (ÂΩìÂâçÁä∂ÊÄÅ: {db_repo.status.value})")
    # Reset status to PENDING to allow it to be picked up (works for FAILED and SKIPPED)
    db_repo.status = ProcessingStatus.PENDING
    db_repo.error_message = None
    db_repo.updated_at = datetime.datetime.now(datetime.UTC)
    db.commit()

    # Trigger a sync task in background to process immediately
    background_tasks.add_task(sync_task, False, True)

    logger.info(f"‚úÖ Â∑≤Â∞Ü‰ªìÂ∫ì {db_repo.repo_name} Ê†áËÆ∞‰∏∫ÂæÖÂ§ÑÁêÜ")
    return {"status": "retrying"}

@app.post("/trigger")
async def trigger(background_tasks: BackgroundTasks, sync_all: bool = False):
    logger.info(f"üéØ ÊâãÂä®Ëß¶ÂèëÂêåÊ≠•‰ªªÂä° (sync_all={sync_all})")
    background_tasks.add_task(sync_task, sync_all)
    return {"status": "triggered"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8002)
